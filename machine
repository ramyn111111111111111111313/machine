Gradient Descent Implemented on multiple features
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.interpolate import pchip
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
class my_gradient:
    def __init__(self, X,Y, lrate, initial_b, weights, niteration):
        self.X = X
        self.Y = Y
        self.lrate = lrate
        self.initial_b = initial_b
        self.weights = weights
        self.niteration = niteration

    
    def predict(updatead_weights, x):
        return (updatead_weights[0] + sum((x * updatead_weights[1:])))


    def compute_error(updatead_weights, X, Y):
        totalError=0
        for i in range(0,len(X)):
            totalError +=(Y[i] - (updatead_weights * X[i] + updatead_weights[0]))**2
        return sum(totalError/float(len(X)))
        
    def gradientStep(weights, X, Y, lrate):
        dm = np.zeros(len(weights))
        N=float(len(X))
        copy_weights = weights
        updatead_weights = weights
        
        for i in range(0, len(X)):
            for j in range(0, len(weights)):
                dm[j] +=-(2/N) * X[i][j] * (Y[i] -(np.dot(weights, X[i])+ updatead_weights[0]))

            updatead_weights = np.array(copy_weights - (lrate * dm))

        return updatead_weights
        
    def gradientRun(X, Y, weights, lrate, numiteration, error_list):
        updatead_weights = weights
        for i in range(numiteration):
            updatead_weights = my_gradient.gradientStep(updatead_weights ,X ,Y ,lrate)
            error = my_gradient.compute_error(updatead_weights, X, Y)
            error_list.append(error)
            print('Iteration number ' , str(i),": The error--> ", error)
        return[updatead_weights,error_list]
    
    def fit(self):
        error_list = []
        [updatead_weights,final_error_list] = my_gradient.gradientRun(self.X, self.Y, self.weights, self.lrate, self.niteration, error_list)
        print(updatead_weights)
        
        return updatead_weights
df = pd.read_csv(r'D:\Work\1- MSA TA-RA\Machine Learning (Dr. Ammar)\Lab Notebooks\Advertising.csv')
df
df = pd.read_csv(r'D:\Work\1- MSA TA-RA\Machine Learning (Dr. Ammar)\Lab Notebooks\Advertising.csv')

Y = np.array(df['sales'])

df.drop(['Unnamed: 0' , 'sales'] , axis = 1, inplace = True)
X = np.c_[np.ones(df.shape[0]), df]
lrate=0.00001
initial_b=0
#--------------------------------
n_features = df.shape[1]
weights = np.zeros(n_features + 1)
#--------------------------------
niteration=100
print(weights)

my_model = my_gradient(X ,Y ,lrate, initial_b, weights, niteration)
model1_weights= my_model.fit()
predicted_value = my_gradient.predict(model1_weights, [1000,300,40])
predicted_value
Ploynomial Rregression using Sklearn for multiple features
np.random.seed(1)
x_1 = np.absolute(np.random.randn(100, 1) * 10)
x_2 = np.absolute(np.random.randn(100, 1) * 30)
y = 2*x_1**2 + 3*x_1 + 2 + np.random.randn(100, 1)*20
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
axes[0].scatter(x_1, y)
axes[1].scatter(x_2, y)
axes[0].set_title("x_1 plotted")
axes[1].set_title("x_2 plotted")
plt.show()
df = pd.read_csv(r'/content/Advertising.csv')
df
X, y = df[["TV", "radio"]], df["sales"]
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=42)
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_train, y_train)
poly_reg_y_predicted = poly_reg_model.predict(X_test)
poly_reg_y_predicted
poly_reg_model.score(X_test,y_test)
Evaluating Model used k-fold cross validation
# load the dataset
df = pd.read_csv(r'/content/Advertising.csv')

# split the dataset into features and target
X = df.drop(['Unnamed: 0' , 'sales'], axis=1)
y = df['sales']

# create a linear regression model
model = LinearRegression()

# create a k-fold cross validation object
kf = KFold(n_splits=10, shuffle=True)

# loop over each fold and train the model
mse_scores = []
i = 0
for train_index, test_index in kf.split(X):
    # split the data into training and test sets
    x_train, x_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # fit the model to the training data
    model.fit(x_train, y_train)

    # make predictions on the test data
    y_pred = model.predict(x_test)

    # calculate the mean squared error for this fold
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)
    print("Error of fold number", str(i) ,"=", str(model.score(x_test,y_test)))
    i += 1
df = pd.read_csv(r'C:\Users\tatas\Downloads\Compressed\archive_11\Players\Players\Fifa22.csv')
df
Logistic Regression from scratch
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
class MyLogisticRegression:
    def __init__(self, lr=0.01, num_iter=100000):
        self.lr = lr
        self.num_iter = num_iter
        
    
    def __sigmoid(self, z):
        return 1 / (1 + np.exp(-z))
    
    def __loss(self, h, y):
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    
    def fit(self, X, y):
        # weights initialization
        self.theta = np.zeros(X.shape[1])
        # Run Gradient Descent
        for i in range(self.num_iter):
            z = np.dot(X, self.theta)
            h = self.__sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / y.size
            self.theta -= self.lr * gradient
            
            z = np.dot(X, self.theta)
            h = self.__sigmoid(z)
            loss = self.__loss(h, y)
            print(loss)
            
            
    def predict_prob(self, X):
        return self.__sigmoid(np.dot(X, self.theta))
    
    def predict(self, X):
        return self.predict_prob(X).round()
load data
iris = datasets.load_iris()
iris.data
iris.target
target_values = set(iris.target)
target_values
Pre-process data target column
X = iris.data
y_list = []
for i in range(0 , len(target_values)):
    y_list.append((iris.target != i) * 1)
    print(y_list[i])
    print('----------------------------------------')
X
#to fit imput data X and binary class vector y you can call the following
model_lists = []
for i in range(0, len(y_list)):
    model = MyLogisticRegression(lr=0.1, num_iter=3000)
    model.fit(X, y_list[i])
    model_lists.append(model)
model_lists
# to predict the class labels of input vector Z you can call
Z=np.array([6.2,3.5,5.4,2.3])
# Z=np.array([2.2,3.5,2.4,0.3])

for j in range(0 , len(model_lists)):
    preds = model_lists[j].predict(Z)
    print(preds)
for j in range(0 , len(model_lists)):
    preds = model_lists[j].predict_prob(Z)
    print(preds)
Naive Baise
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
from sklearn import preprocessing
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Load the training data
df = pd.read_csv('glass.csv')
df
X = df.drop(['Type'] , axis = 1)
X
y = np.array(df['Type'])
X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2)
print(len(X_train))
print(len(X_test))
clf = GaussianNB()
clf.fit(X_train, y_train)
# Make predictions on the testing set
y_pred = clf.predict(X_test)
# Evaluate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)
recall = recall_score(y_test, y_pred)
print('Recall:', recall)
precision = precision_score(y_test, y_pred)
print('Precision:', precision)
f1 = f1_score(y_test,y_pred)
print('F1_score:', f1)
# Create confusion matrix
cm = confusion_matrix(y_test, y_pred)
cm
svm
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define the SVM classifier with specific parameters
svm = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)

# Train the SVM classifier on the training set
svm.fit(X_train, y_train)

# Predict the labels of the testing set using the trained SVM classifier
y_pred = svm.predict(X_test)

# Calculate the accuracy score of the SVM classifier on the testing set
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy:", accuracy)

NN
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from keras.models import Sequential
from keras.layers import Dense

# Load the Iris dataset
iris_data = load_iris()
X = iris_data.data
y = iris_data.target.reshape(-1, 1)

# One-hot encode the target variable
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y)

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create the neural network
model = Sequential()
model.add(Dense(10, input_dim=4, activation='relu'))  # 1st hidden layer with 10 neurons, input dimension of 4 (features)
model.add(Dense(5, activation='relu'))  # 2nd hidden layer with 5 neurons
model.add(Dense(3, activation='softmax'))  # Output layer with 3 neurons (output classes)

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=5, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')
 Image Dataset
Minst
import matplotlib.pyplot as plt
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Flatten
from keras.utils import to_categorical
# Load the MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train.reshape((X_train.shape[0], 28 * 28)).astype('float32') / 255
X_test = X_test.reshape((X_test.shape[0], 28 * 28)).astype('float32') / 255
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
# Visualize a sample image for each label
num_classes = 10
samples_per_class = 1
plt.figure(figsize=(10, 5))
for y in range(num_classes):
    idxs = np.flatnonzero(np.argmax(y_train, axis=1) == y)
    idxs = np.random.choice(idxs, samples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt_idx = i * num_classes + y + 1
        plt.subplot(samples_per_class, num_classes, plt_idx)
        plt.imshow(X_train[idx].reshape(28, 28), cmap='gray')
        plt.axis('off')
        if i == 0:
            plt.title(str(y))
plt.show()
t# Create the neural network
model = Sequential()
model.add(Dense(512, activation='relu', input_shape=(28 * 28,)))
model.add(Dense(256, activation='relu'))
model.add(Dense(10, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Loss: {loss}, Accuracy: {accuracy}')
